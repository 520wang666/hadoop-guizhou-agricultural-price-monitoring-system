# 开发环境配置说明

## 1. 环境要求

### 1.1 硬件要求

| 组件 | 最低配置 | 推荐配置 |
|------|----------|----------|
| CPU | 4核 | 8核+ |
| 内存 | 16GB | 32GB+ |
| 硬盘 | 200GB | 500GB+ |
| 网络 | 100Mbps | 1Gbps |

### 1.2 软件要求

| 软件 | 版本 | 说明 |
|------|------|------|
| 操作系统 | CentOS 7.x / Ubuntu 20.04+ | 服务器操作系统 |
| JDK | 11+ | Java开发环境 |
| Python | 3.8+ | 爬虫和数据处理 |
| Node.js | 16+ | 前端开发 |
| MySQL | 8.0+ | 元数据存储 |
| Hadoop | 3.3.x | 分布式存储计算 |
| Hive | 3.1.x | 数据仓库仓库 |
| Spark | 3.3.x | 分布式计算 |
| Kafka | 2.8.x | 消息队列 |
| Redis | 6.x | 缓存 |
| Maven | 3.6+ | Java项目构建 |
| Docker | 20.10+ | 容器化部署 |

---

## 2. 开发工具安装

### 2.1 JDK安装

```bash
# 下载JDK 11
wget https://download.oracle.com/java/11/latest/jdk-11_linux-x64_bin.tar.gz

# 解压
tar -zxvf jdk-11_linux-x64_bin.tar.gz -C /usr/local/

# 配置环境变量
cat >> /etc/profile << 'EOF'
export JAVA_HOME=/usr/local/jdk-11
export PATH=$JAVA_HOME/bin:$PATH
EOF

# 使配置生效
source /etc/profile

# 验证安装
java -version
```

### 2.2 Python环境配置

```bash
# 安装Python 3.8
yum install -y python38 python-devel

# 安装pip
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python3 get-pip.py

# 创建虚拟环境
cd /path/to/project
python3 -m venv venv

# 激活虚拟环境
source venv/bin/activate

# 安装依赖
pip install -r crawler/scrapy_crawler/requirements.txt
```

### 2.3 Node.js安装

```bash
# 使用nvm安装Node.js
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
source ~/.bashrc

# 安装Node.js 16
nvm install 16
nvm use 16

# 验证安装
node -v
npm -v
```

### 2.4 Maven安装

```bash
# 下载Maven
wget https://dlcdn.apache.org/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz

# 解压
tar -zxvf apache-maven-3.8.6-bin.tar.gz -C /usr/local/

# 配置环境变量
cat >> /etc/profile << 'EOF'
export MAVEN_HOME=/usr/local/apache-maven-3.8.6
export PATH=$MAVEN_HOME/bin:$PATH
EOF

# 使配置生效
source /etc/profile

# 验证安装
mvn -v
```

---

## 3. Hadoop集群配置

### 3.1 Hadoop安装

```bash
# 下载Hadoop 3.3.x
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz

# 解压
tar -zxvf hadoop-3.3.4.tar.gz -C /usr/local/
cd /usr/local/hadoop-3.3.4

# 配置环境变量
cat >> /etc/profile << 'EOF'
export HADOOP_HOME=/usr/local/hadoop-3.3.4
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
EOF

source /etc/profile
```

### 3.2 HDFS配置

**core-site.xml**
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/hadoop-3.3.4/tmp</value>
    </property>
</configuration>
```

**hdfs-site.xml**
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/usr/local/hadoop-3.3.4/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/usr/local/hadoop-3.3.4/hdfs/datanode</value>
    </property>
</configuration>
```

### 3.3 启动Hadoop

```bash
# 格式化NameNode（首次运行）
hdfs namenode -format

# 启动HDFS
start-dfs.sh

# 验证
jps
hdfs dfs -ls /
```

---

## 4. Hive配置

### 4.1 Hive安装

```bash
# 下载Hive 3.1.x
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# 解压
tar -zxvf apache-hive-3.1.3-bin.tar.gz -C /usr/local/
cd /usr/local/apache-hive-3.1.3-bin

# 配置环境变量
cat >> /etc/profile << 'EOF'
export HIVE_HOME=/usr/local/apache-hive-3.1.3-bin
export PATH=$HIVE_HOME/bin:$PATH
EOF

source /etc/profile
```

### 4.2 Hive配置

**hive-site.xml**
```xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive123</value>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
</configuration>
```

### 4.3 初始化Hive

```bash
# 下载MySQL JDBC驱动
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar
cp mysql-connector-java-8.0.28.jar $HIVE_HOME/lib/

# 初始化元数据
schematool -dbType mysql -initSchema

# 启动Hive
hive
```

---

## 5. Spark配置

### 5.1 Spark安装

```bash
# 下载Spark 3.3.x
wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz

# 解压
tar -zxvf spark-3.3.2-bin-hadoop3.tgz -C /usr/local/
cd /usr/local/spark-3.3.2-bin-hadoop3

# 配置环境变量
cat >> /etc/profile << 'EOF'
export SPARK_HOME=/usr/local/spark-3.3.2-bin-hadoop3
export PATH=$SPARK_HOME/bin:$PATH
EOF

source /etc/profile
```

### 5.2 Spark配置

**spark-defaults.conf**
```properties
spark.master                     yarn
spark.executor.memory            2g
spark.driver.memory              1g
spark.executor.cores            2
spark.default.parallelism        4
spark.sql.warehouse.dir          hdfs://localhost:9000/user/hive/warehouse
```

### 5.3 验证Spark

```bash
# 启动Spark Shell
spark-shell

# 测试
sc.parallelize(1 to 10).count()
```

---

## 6. Kafka配置

### 6.1 Kafka安装

```bash
# 下载Kafka 2.8.x
wget https://downloads.apache.org/kafka/2.8.2/ kafka_2.12-2.8.2.tgz

# 解压
tar -zxvf kafka_2.12-2.8.2.tgz -C /usr/local/
cd /usr/local/kafka_2.12-2.8.2
```

### 6.2 启动Kafka

```bash
# 启动Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties &

# 启动Kafka
bin/kafka-server-start.sh config/server.properties &

# 创建主题
bin/kafka-topics.sh --create --topic price-data --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

---

## 7. MySQL配置

### 7.1 MySQL安装

```bash
# 安装MySQL
yum install -y mysql-server

# 启动MySQL
systemctl start mysqld
systemctl enable mysqld

# 获取临时密码
grep 'temporary password' /var/log/mysqld.log

# 登录MySQL
mysql -u root -p

# 修改密码
ALTER USER 'root'@'localhost' IDENTIFIED BY 'Root123!';

# 创建数据库
CREATE DATABASE agriculture CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
CREATE DATABASE hive CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

# 创建用户
CREATE USER 'agriculture'@'%' IDENTIFIED BY 'Agriculture123!';
CREATE USER 'hive'@'%' IDENTIFIED BY 'Hive123!';

# 授权
GRANT ALL PRIVILEGES ON agriculture.* TO 'agriculture'@'%';
GRANT ALL PRIVILEGES ON hive.* TO 'hive'@'%';

FLUSH PRIVILEGES;
```

### 7.2 初始化数据库

```bash
# 导入初始化脚本
mysql -u agriculture -p agriculture < backend/mysql/init.sql
```

---

## 8. Redis配置

### 8.1 Redis安装

```bash
# 安装Redis
yum install -y redis

# 启动Redis
systemctl start redis
systemctl enable redis

# 验证
redis-cli ping
```

### 8.2 Redis配置

**redis.conf**
```properties
bind 0.0.0.0
port 6379
requirepass Redis123!
maxmemory 2gb
maxmemory-policy allkeys-lru
```

---

## 9. 后端服务配置

### 9.1 Spring Boot配置

**application.yml**
```yaml
server:
  port: 8080

spring:
  application:
    name: agriculture-price-api
  
  datasource:
    url: jdbc:mysql://localhost:3306/agriculture?useUnicode=true&characterEncoding=utf8
    username: agriculture
    password: Agriculture123!
    driver-class-name: com.mysql.cj.jdbc.Driver
  
  redis:
    host: localhost
    port: 6379
    password: Redis123!

# Hive配置
hive:
  url: jdbc:hive2://localhost:10000/default
  username: hive
  password: 

# HBase配置
hbase:
  quorum: localhost
  port: 2181

# 日志配置
logging:
  level:
    root: INFO
    com.agriculture: DEBUG
  file:
    name: logs/backend/spring-boot-api.log
```

### 9.2 启动后端服务

```bash
cd backend/spring-boot-api

# 编译打包
mvn clean package -DskipTests

# 启动服务
java -jar target/agriculture-price-api-1.0.0.jar

# 或使用Maven启动
mvn spring-boot:run
```

---

## 10. 前端服务配置

### 10.1 Vue3项目配置

**.env.development**
```properties
VITE_API_BASE_URL=http://localhost:8080/api/v1
VITE_APP_TITLE=农产品价格监测系统
```

**.env.production**
```properties
VITE_API_BASE_URL=https://api.example.com/api/v1
VITE_APP_TITLE=农产品价格监测系统
```

### 10.2 启动前端服务

```bash
cd frontend/vue3-web

# 安装依赖
npm install

# 启动开发服务器
npm run dev

# 编译打包
npm run build
```

---

## 11. 爬虫服务配置

### 11.1 Scrapy配置

**settings.py**
```python
# Scrapy设置
BOT_NAME = 'scrapy_crawler'
SPIDER_MODULES = ['scrapy_crawler.spiders']
NEWSPIDER_MODULE = 'scrapy_crawler.spiders'

# 并发设置
CONCURRENT_REQUESTS = 16
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# 下载延迟
设置DOWNLOAD_DELAY = 2

# Redis设置
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = 0

# 日志设置
LOG_LEVEL = 'INFO'
LOG_FILE = 'logs/crawler/scrapy_crawler.log'

# User-Agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# 中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_crawler.middlewares.ProxyMiddleware': 400,
    'scrapy_crawler.middlewares.UserAgentMiddleware': 401,
}

# 管道
ITEM_PIPELINES = {
    'scrapy_crawler.pipelines.KafkaPipeline': 300,
    'scrapy_crawler.pipelines.ValidationPipeline': 200,
}
```

### 11.2 启动爬虫

```bash
cd crawler/scrapy_crawler

# 启动Redis（用于分布式爬虫）
redis-server

# 启动爬虫
scrapy crawl ecommerce_spider

# 分布式爬虫
scrapy crawl ecommerce_spider -s JOBDIR=false
```

---

## 12. Airflow配置

### 12.1 Airflow安装

```bash
# 安装Airflow
pip install apache-airflow[celery,postgres,hive,hdfs,spark,kafka]==2.5.0

# 初始化数据库
airflow db init

# 创建用户
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin123

# 启动Airflow
airflow webserver -p 8080 &
airflow scheduler &
airflow celery worker &
```

### 12.2 Airflow配置

**airflow.cfg**
```ini
[core]
dags_folder = /path/to/hadoop/airflow/dags
executor = CeleryExecutor
sql_alchemy_conn = mysql://airflow:Airflow123@localhost:3306/airflow

[webserver]
web_server_port = 8080

[scheduler]
catchup_by_default = False

[celery]
broker_url = redis://:Redis123!@localhost:6379/0
result_backend = db+mysql://airflow:Airflow123@localhost:3306/airflow
```

---

## 13. Docker部署（可选）

### 13.1 Docker Compose配置

**docker-compose.yml**
```yaml
version: '3.8'

services:
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: Root123!
      MYSQL_DATABASE: agriculture
    ports:
      - "3306:3306"
    volumes:
      - mysql-data:/var/lib/mysql

  redis:
    image: redis:6
    command: redis-server --requirepass Redis123!
    ports:
      - "6379:6379"

  backend:
    build: ./backend/spring-boot-api
    ports:
      - "8080:8080"
    depends_on:
      - mysql
      - redis

  frontend:
    build: ./frontend/vue3-web
    ports:
      - "80:80"
    depends_on:
      - backend

volumes:
  mysql-data:
```

### 13.2 启动服务

```bash
# 启动所有服务
docker-compose up -d

# 查看日志
docker-compose logs -f

# 停止服务
docker-compose down
```

---

## 14. 环境变量配置

### 14.1 创建.env文件

```bash
# 复制环境变量模板
cp .env.example .env

# 编辑环境变量
vim .env
```

### 14.2 环境变量示例

**.env**
```properties
# 数据库配置
DB_HOST=localhost
DB_PORT=3306
DB_NAME=agriculture
DB_USER=agriculture
DB_PASSWORD=Agriculture123!

# Redis配置
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=Redis123!

# Hadoop配置
HADOOP_NAMENODE=localhost:9000
HIVE_HOST=localhost
HIVE_PORT=10000

# Kafka配置
KAFKA_BROKERS=localhost:9092

# 日志配置
LOG_LEVEL=INFO
LOG_PATH=logs/
```

---

## 15. 验证环境

### 15.1 健康检查脚本

```bash
#!/bin/bash

echo "检查Java..."
java -version

echo "检查Python..."
python3 --version

echo "检查Node.js..."
..."
node -v

echo "检查Hadoop..."
hdfs dfs -ls /

echo "检查Hive..."
hive -e "SHOW DATABASES;"

